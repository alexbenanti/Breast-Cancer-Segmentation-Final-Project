{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIA Final",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "g-o8Yee8m-_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set-up**"
      ],
      "metadata": {
        "id": "SwmNoXyqMyEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchgeometry\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "from sklearn import model_selection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchgeometry.losses import dice_loss\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF"
      ],
      "metadata": {
        "id": "jfsUn1o2KZF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgkVIGUqskRv"
      },
      "outputs": [],
      "source": [
        "# The original paper set aside images which came from the following labs \n",
        "# as their test data. We follow their example.\n",
        "testing_sites = ['OL', 'LL', 'C8', 'BH', 'AR', 'A7', 'A1']\n",
        "\n",
        "# The dataset is loaded from google drive\n",
        "rgbs_dir = '/content/drive/MyDrive/0_Public-data-Amgad2019_0.25MPP/rgbs_colorNormalized'\n",
        "masks_dir = '/content/drive/MyDrive/0_Public-data-Amgad2019_0.25MPP/masks'\n",
        "\n",
        "# Splitting training and testing data\n",
        "training_names = []\n",
        "test_names = []\n",
        "\n",
        "# The rgbs and masks have the exact same filenames, so we do not need to treat\n",
        "# them separately\n",
        "for filename in os.listdir(rgbs_dir):\n",
        "    # The lab location exists in the filename at the fifth and sixth characters.\n",
        "    if filename[5:7] in testing_sites:\n",
        "      test_names += [filename]\n",
        "    else:\n",
        "      training_names += [filename]\n",
        "\n",
        "val_size = 22\n",
        "train_size = 60\n",
        "\n",
        "train_names, val_names = model_selection.train_test_split(training_names,\n",
        "                                                          train_size = train_size,\n",
        "                                                          test_size = val_size,\n",
        "                                                          random_state = 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "JCe9dsT-MsMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.segmentation.deeplabv3_resnet50(num_classes = 5, \n",
        "                                                     pretrained_backbone = True)\n",
        "device = torch.device(\"cuda:0\")\n",
        "model.to(device)\n",
        "\n",
        "# There are two children in the DeepLab model, the encoder and the decoder. For\n",
        "# finetuning, we freeze the encoder layers\n",
        "for i, child in enumerate(model.children()):\n",
        "  if i == 0:\n",
        "    for param in child.parameters():\n",
        "      param.requires_grad = False"
      ],
      "metadata": {
        "id": "RANP_LFvDvf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auxiliary Code**"
      ],
      "metadata": {
        "id": "i0z_xhDnM60t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The masks in the dataset are annotated with labels from 1 to 21. The original\n",
        "# paper specifies how to separate these classes into the five classes used for \n",
        "# our purposes.\n",
        "\n",
        "def change_class_vals(mask):\n",
        "   \n",
        "  for i in range(1, 22):\n",
        "\n",
        "    if i == 1 or i == 19 or i == 20:\n",
        "      mask[mask == i] = 0\n",
        "    elif i == 2:\n",
        "      mask[mask == i] = 1\n",
        "    elif i == 3 or i == 10 or i == 11 or i == 14:\n",
        "      mask[mask == i] = 2\n",
        "    elif i == 4:\n",
        "      mask[mask == i] = 3\n",
        "    else:\n",
        "      mask[mask == i] = 4\n",
        "\n",
        "  return mask\n"
      ],
      "metadata": {
        "id": "1e--_9pxJyrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We ran this code once to determine the mean and std of each channel\n",
        "# for our training data set\n",
        "\n",
        "'''\n",
        "class CalcForNorm(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, filenames):\n",
        "\n",
        "        self.filenames = filenames\n",
        "    \n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.filenames) \n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        rgb_name = os.path.join(rgbs_dir, self.filenames[index])\n",
        "        rgb = TF.to_tensor(Image.open(rgb_name))\n",
        "\n",
        "        return rgb\n",
        "    \n",
        "dataset = CalcForNorm(training_names)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, \n",
        "                                          batch_size=1, \n",
        "                                          num_workers=1, \n",
        "                                          shuffle=False)\n",
        "\n",
        "channel_sums = [0] * 3\n",
        "channel_squared_sums = [0] * 3\n",
        "total_pixels = 0\n",
        "\n",
        "for rgb in data_loader:\n",
        "  rgb = rgb.squeeze(0)\n",
        "  total_pixels += rgb.shape[1] * rgb.shape[2]\n",
        "\n",
        "  for i in range(3):\n",
        "    channel_sums[i] += torch.sum(rgb[i,:,:])\n",
        "    channel_squared_sums[i] += torch.sum(rgb[i,:,:] ** 2)\n",
        "\n",
        "means = [0] * 3\n",
        "stds = [0] * 3\n",
        "\n",
        "for i in range(3):\n",
        "   means[i] = channel_sums[i] / total_pixels\n",
        "   stds[i] = (channel_squared_sums[i] / total_pixels - means[i]**2) ** 0.5\n",
        "\n",
        "print(means, stds)\n",
        "'''"
      ],
      "metadata": {
        "id": "osTnUMnvcTe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We ran this code once to determine the class weights for our cross\n",
        "# entropy loss function. Classes are weighted according to a formula in the \n",
        "# original paper. Less common classes receive more weight.\n",
        "\n",
        "'''\n",
        "weights = [0] * 5\n",
        "# N is the total amount of pixels in the dataset\n",
        "N = 0\n",
        "\n",
        "# Find the total number of pixels of each class in the dataset\n",
        "for filename in train_names:\n",
        "  mask = cv2.imread(os.path.join(masks_dir, filename))[:,:,0]\n",
        "  mask = change_class_vals(mask)\n",
        "  N += np.size(mask)\n",
        "  weights[0] += np.size(mask[mask == 0])\n",
        "  weights[1] += np.size(mask[mask == 1])\n",
        "  weights[2] += np.size(mask[mask == 2])\n",
        "  weights[3] += np.size(mask[mask == 3])\n",
        "  weights[4] += np.size(mask[mask == 4])\n",
        "\n",
        "for i in range(len(weights)):\n",
        "  weights[i] = 1 - weights[i] / N\n",
        "\n",
        "print(weights)\n",
        "'''"
      ],
      "metadata": {
        "id": "_G4hwSBa-A9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss and Optimizer**"
      ],
      "metadata": {
        "id": "uEmEojLcPJVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [0.4905573355962124, 0.695194226695798, 0.9165659564679096, 0.9391179214116163, 0.9585645598284636]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(torch.FloatTensor(weights).to(device))\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "#                                                      threshold = 0.01, \n",
        "#                                                      patience = 3)"
      ],
      "metadata": {
        "id": "UWRT2P-29-v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data**"
      ],
      "metadata": {
        "id": "4eT4vf2g3RUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the channel means and stds of our dataset,\n",
        "# used for normalization when training from scratch\n",
        "# means = [0.7395, 0.5253, 0.7026]\n",
        "# stds = [0.1912, 0.2321, 0.1716]\n",
        "\n",
        "# These are the channel means and stds of the dataset used to pretrain our models,\n",
        "# used for normalization when fine-tuning\n",
        "means = [0.485, 0.456, 0.406]\n",
        "stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "class MakeDataset(Dataset):\n",
        "\n",
        "  def __init__(self, filenames, training):\n",
        "\n",
        "        self.training = training\n",
        "        self.filenames = filenames\n",
        "\n",
        "  def __len__(self):\n",
        "      \n",
        "      return len(self.filenames)\n",
        "\n",
        "  # Basic data augmentation for training data\n",
        "  def transform_train(self, rgb, mask):\n",
        "\n",
        "        # Random crop\n",
        "        i, j, h, w = torchvision.transforms.RandomCrop.get_params(\n",
        "            rgb, output_size = (768, 768))\n",
        "        rgb = TF.crop(rgb, i, j, h, w)\n",
        "        mask = TF.crop(mask, i, j, h, w)\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        if random.random() > 0.5:\n",
        "            rgb = TF.hflip(rgb)\n",
        "            mask = TF.hflip(mask)\n",
        "\n",
        "        # Random vertical flipping\n",
        "        if random.random() > 0.5:\n",
        "            rgb = TF.vflip(rgb)\n",
        "            mask = TF.vflip(mask)\n",
        "\n",
        "        # Transform to tensor and normalize the rgb\n",
        "        rgb = TF.to_tensor(rgb)\n",
        "        rgb = TF.normalize(rgb, means, stds)\n",
        "\n",
        "        mask = TF.to_tensor(mask)\n",
        "\n",
        "        return rgb, mask\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "      \n",
        "        rgb_name = os.path.join(rgbs_dir, self.filenames[index])\n",
        "        mask_name = os.path.join(masks_dir, self.filenames[index])\n",
        "        \n",
        "        rgb = Image.open(rgb_name)\n",
        "        mask = Image.open(mask_name).convert('I;16')\n",
        "        \n",
        "        if self.training :\n",
        "\n",
        "          rgb, mask = self.transform_train(rgb, mask)\n",
        "          mask = change_class_vals(mask.squeeze(0)).long()\n",
        "\n",
        "          return {'rgb': rgb, 'mask': mask}\n",
        "\n",
        "        else :\n",
        "\n",
        "          rgb = TF.to_tensor(TF.center_crop(rgb, output_size = (768, 768)))\n",
        "          rgb = TF.normalize(rgb, means, stds)\n",
        "\n",
        "          mask = TF.to_tensor(TF.center_crop(mask, output_size = (768, 768)))\n",
        "          mask = change_class_vals(mask.squeeze(0)).long()\n",
        "\n",
        "          return {'rgb': rgb, 'mask': mask}\n",
        "\n",
        "train_data = MakeDataset(train_names, training = True)\n",
        "val_data = MakeDataset(val_names, training = False)\n",
        "\n",
        "batch_size = 12\n",
        "\n",
        "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, \n",
        "                          shuffle = True, num_workers = 2)\n",
        "val_loader = DataLoader(dataset = val_data, batch_size = batch_size, \n",
        "                        shuffle = True, num_workers = 2)"
      ],
      "metadata": {
        "id": "_oOOduYVE5lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "V1iwFfW0UsBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_acc(pred, truth):\n",
        "  pred = torch.argmax(F.softmax(pred, dim=1), dim = 1).squeeze(1)\n",
        "  acc = 0\n",
        "\n",
        "  for i in range(len(pred)):\n",
        "    acc += (pred[i] == truth[i]).sum() / torch.numel(pred[i])\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "x8WRAvz_5-vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "\n",
        "train_running_loss_history = []\n",
        "validation_running_loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # We track the loss, the dice loss, and the pixelwise \n",
        "  # accuracy for each epoch\n",
        "  train_loss_running = 0.0\n",
        "  val_loss_running = 0.0\n",
        "  train_dice_running = 0.0\n",
        "  val_dice_running = 0.0\n",
        "  train_pixel_running = 0.0\n",
        "  val_pixel_running = 0.0\n",
        "\n",
        "  model.train()\n",
        " \n",
        "  for i, batch in enumerate(train_loader):\n",
        "    x_train = batch['rgb'].to(device)\n",
        "    y_train = batch['mask'].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(x_train)['out']\n",
        "\n",
        "    # loss = 0.3 * dice_loss(y_pred, y_train) + 0.7 * criterion(y_pred, y_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    train_loss_running += loss.item() * len(y_pred)\n",
        "    train_dice_running += dice_loss(y_pred, y_train) * len(y_pred)\n",
        "    train_pixel_running += pixel_acc(y_pred, y_train)\n",
        "     \n",
        "  with torch.no_grad():\n",
        "      \n",
        "    model.eval()\n",
        "      \n",
        "    for ith_batch, sample_batched in enumerate(val_loader):\n",
        "        x_val = sample_batched['rgb'].to('cuda')\n",
        "        y_val = sample_batched['mask'].to('cuda')\n",
        "          \n",
        "        y_pred = model(x_val)['out']\n",
        "\n",
        "        # val_loss = 0.3 * dice_loss(y_pred, y_val) + 0.7 * criterion(y_pred, y_val)\n",
        "        val_loss = criterion(y_pred, y_val)\n",
        "        val_loss_running += val_loss.item() * len(y_pred)\n",
        "        val_dice_running += dice_loss(y_pred, y_val) * len(y_pred)\n",
        "        val_pixel_running += pixel_acc(y_pred, y_val)\n",
        "\n",
        "    print(\"================================================================================\")\n",
        "    print(\"Epoch {} completed\".format(epoch + 1))\n",
        "      \n",
        "    train_loss = train_loss_running / train_size\n",
        "    val_loss = val_loss_running / val_size\n",
        "    train_dice_loss = train_dice_running / train_size\n",
        "    val_dice_loss = val_dice_running / val_size\n",
        "    train_pixel_acc = train_pixel_running / train_size\n",
        "    val_pixel_acc = val_pixel_running / val_size\n",
        "      \n",
        "    print(\"Training loss: {}\".format(train_loss))\n",
        "    print(\"Training DICE loss: {}\".format(train_dice_loss))\n",
        "    print(\"Training Pixelwise Accuracy: {}\".format(train_pixel_acc))\n",
        "    print(\"Validation loss: {}\".format(val_loss))\n",
        "    print(\"Validation DICE loss: {}\".format(val_dice_loss))\n",
        "    print(\"Validation Pixelwise Accuracy: {}\".format(val_pixel_acc))\n",
        "    print(\"================================================================================\")\n",
        "    train_running_loss_history.append(train_loss)\n",
        "    validation_running_loss_history.append(val_loss)\n",
        "  \n",
        "  #scheduler.step(train_loss)\n",
        "\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "g8raomIKE6Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "DQHInhp-UzDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_class_acc(pred, truth, n):\n",
        "  pred = torch.argmax(F.softmax(pred, dim=1), dim = 1).squeeze(1)\n",
        "  acc = 0\n",
        "  num_cases = len(pred)\n",
        "\n",
        "  for i in range(len(pred)):\n",
        "    class_size = torch.numel(truth[i][truth[i] == n])\n",
        "    # If there are no pixels of the given class in a mask, we need to discount\n",
        "    # that mask when we compute pixel accuracy \n",
        "    if class_size > 0 :\n",
        "      matches = pred[i][pred[i] == truth[i]]\n",
        "      acc += (matches == n).sum() / class_size\n",
        "    else :\n",
        "      num_cases -= 1\n",
        "\n",
        "  return (acc, num_cases)"
      ],
      "metadata": {
        "id": "KbiTF1diEqRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 69\n",
        "\n",
        "test_data = MakeDataset(test_names, training = False)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, \n",
        "                         shuffle = True, num_workers = 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "      \n",
        "  model.eval()\n",
        "      \n",
        "  test_dice_running = 0.0\n",
        "  test_pixel_running = 0.0\n",
        "  class_pixel_running = [0] * 5\n",
        "  num_cases_per_class = [0] * 5\n",
        "\n",
        "  for ith_batch, sample_batched in enumerate(test_loader):\n",
        "      x_test = sample_batched['rgb'].to('cuda')\n",
        "      y_test = sample_batched['mask'].to('cuda')\n",
        "          \n",
        "      y_pred = model(x_test)['out']\n",
        "\n",
        "      test_dice_running += dice_loss(y_pred, y_test) * len(y_pred)\n",
        "      test_pixel_running += pixel_acc(y_pred, y_test)\n",
        "      for i in range(5) :\n",
        "        acc, num_cases = pixel_class_acc(y_pred, y_test, i)\n",
        "        class_pixel_running[i] += acc\n",
        "        num_cases_per_class[i] += num_cases\n",
        "\n",
        "print(\"DICE Score: {}\".format(1 - (test_dice_running / test_size)))\n",
        "print(\"Pixelwise Accuracy: {}\".format(test_pixel_running / test_size))\n",
        "for i in range(5) :\n",
        "  print(\"Pixelwise Accuracy for Class {}: {}\".format(i, class_pixel_running[i] / \n",
        "                                                     num_cases_per_class[i]))"
      ],
      "metadata": {
        "id": "JWrYVxzHV93p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ttqg7r6ZExH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}